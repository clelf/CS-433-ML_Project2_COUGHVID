{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rfi7I_PepMI1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GroupKFold\n",
    "\n",
    "## Custom libraries\n",
    "import index_helpers as ih\n",
    "from importlib import reload\n",
    "import data_transformations as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofGc4Xd4pMI3"
   },
   "outputs": [],
   "source": [
    "reload(dt)\n",
    "reload(ih)\n",
    "## Import, index, and split\n",
    "\n",
    "segmentation = True\n",
    "fine_segmentation = True\n",
    "exclude_expert = False\n",
    "exclude_meta_data= False\n",
    "\n",
    "normalization=False\n",
    "power=False\n",
    "\n",
    "\n",
    "df = ih.read_and_merge_data(segmentation, fine_segmentation, exclude_expert, exclude_meta_data=False)\n",
    "df = dt.transformation_call(df, normalization, power)\n",
    "\n",
    "if segmentation:\n",
    "  df = ih.index_df_by_person(df)\n",
    "else:\n",
    "  df = df.set_index(['File_Name'])\n",
    "df = ih.categorical_float_to_int(df) #d'ailleurs elle ne sert Ã  rien cette fonction\n",
    "df = ih.categorical_to_dummy(df, include_expert_as_dummies=False, exclude_meta_data=False)\n",
    "df1, df2, df3 = ih.separate_expert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2zN59Q2HOFr",
    "outputId": "b546e2ff-bb8d-4ab2-b7d5-9d3cb17220aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  88 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  88 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   35.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "aucs = []\n",
    "for df_ in df1, df2, df3, df:\n",
    "    if segmentation:\n",
    "    X_train, X_val, y_train, y_val = ih.train_test_split_on_index(features = df_.drop(\"Label\", axis=1),\n",
    "                                                             label = df_[\"Label\"])\n",
    "    groups = y_train.reset_index()['File_Name_split']\n",
    "    else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df1.drop(\"Label\", axis=1), df1[\"Label\"], test_size=0.2)\n",
    "    groups = y_train.reset_index()['File_Name']\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    ## Make pipeline - name classifier \"clf\"\n",
    "    clf_pipeline = Pipeline([(\"st_scaler\", StandardScaler()),\n",
    "                          (\"clf\", KNeighborsClassifier())])\n",
    "\n",
    "    ## Use \"clf__\" in order to correctly assign parameters to the clf object\n",
    "    clf_param_grid = {'clf__n_neighbors': list(range(1,30)),\n",
    "                    'clf__leaf_size': list(range(1,50)),\n",
    "                    'clf__p': [1, 2]}\n",
    "\n",
    "\n",
    "    ## Instantiate GroupKFold to avoid data leakage - to be passed to cv\n",
    "    gkf=GroupKFold(n_splits=2)\n",
    "\n",
    "    ## Set up Randomized search CV --> modulate n_iter for \"quicker\" results\n",
    "    clf_rand_auc = RandomizedSearchCV(estimator=clf_pipeline,\n",
    "                                    param_distributions=clf_param_grid,\n",
    "                                    cv=gkf, scoring='roc_auc', verbose=1, n_jobs=2, n_iter=50)\n",
    "\n",
    "    ## Perform Group K-Cross-validation\n",
    "    clf_rand_auc.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "    ## Prediction\n",
    "    pred = clf_rand_auc.predict(X_val)\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "  \n",
    "    ## AUC\n",
    "    aucs.append(auc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KNN_Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
